{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13502012,"sourceType":"datasetVersion","datasetId":8572763}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.037410Z","iopub.execute_input":"2025-10-26T19:14:04.038286Z","iopub.status.idle":"2025-10-26T19:14:04.416660Z","shell.execute_reply.started":"2025-10-26T19:14:04.038257Z","shell.execute_reply":"2025-10-26T19:14:04.415595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('/kaggle/input/gpt-training-data/input.txt', 'r', encoding='utf-8') as f:   #utf-8 enables reading of special characters that are skipped otherwise\n    text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.418710Z","iopub.execute_input":"2025-10-26T19:14:04.419220Z","iopub.status.idle":"2025-10-26T19:14:04.448531Z","shell.execute_reply.started":"2025-10-26T19:14:04.419185Z","shell.execute_reply":"2025-10-26T19:14:04.447545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(text)    #no. of characters we are dealing with","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.449569Z","iopub.execute_input":"2025-10-26T19:14:04.449909Z","iopub.status.idle":"2025-10-26T19:14:04.458702Z","shell.execute_reply.started":"2025-10-26T19:14:04.449877Z","shell.execute_reply":"2025-10-26T19:14:04.457578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(text[:100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.459804Z","iopub.execute_input":"2025-10-26T19:14:04.460308Z","iopub.status.idle":"2025-10-26T19:14:04.475514Z","shell.execute_reply.started":"2025-10-26T19:14:04.460268Z","shell.execute_reply":"2025-10-26T19:14:04.474344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chars = sorted(list(set(text)))   #to see all the characters the model is going to encounter\nvocab_size = len(chars)\nprint(''.join(chars))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.477852Z","iopub.execute_input":"2025-10-26T19:14:04.478120Z","iopub.status.idle":"2025-10-26T19:14:04.510524Z","shell.execute_reply.started":"2025-10-26T19:14:04.478097Z","shell.execute_reply":"2025-10-26T19:14:04.509539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' \nNext step is to tokenize the text.\nSimple baseline tokenization would be taking the ascii value of each character\nand keeping all the values together in a list to create a vector.\nThen use libraries like tiktoken and more complex tokenizers.\n'''\n\nstoi = {ch:i for i, ch in enumerate(chars)}\nitos = {i:ch for i, ch in enumerate(chars)}\n\nencode = lambda s: [stoi[c] for c in s]    #Lambda function just applies the encoding to all the strings\ndecode = lambda l: ''.join([itos[i] for i in l])    #This lambda function does the reverse.\n\nencode('aaaabbbb')\n# decode(encode('hello'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.511502Z","iopub.execute_input":"2025-10-26T19:14:04.511820Z","iopub.status.idle":"2025-10-26T19:14:04.533995Z","shell.execute_reply.started":"2025-10-26T19:14:04.511797Z","shell.execute_reply":"2025-10-26T19:14:04.532904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndata = torch.tensor(encode(text), dtype=torch.long)   #To simply create a pytorch tensor for entering into model training.\ndata.shape\n# print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:04.535091Z","iopub.execute_input":"2025-10-26T19:14:04.535625Z","iopub.status.idle":"2025-10-26T19:14:09.338747Z","shell.execute_reply.started":"2025-10-26T19:14:04.535572Z","shell.execute_reply":"2025-10-26T19:14:09.337850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We'll now split into training and validation data for the model\n\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:14:09.339893Z","iopub.execute_input":"2025-10-26T19:14:09.340321Z","iopub.status.idle":"2025-10-26T19:14:09.360574Z","shell.execute_reply.started":"2025-10-26T19:14:09.340297Z","shell.execute_reply":"2025-10-26T19:14:09.359388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We'll define the block size of characters that we pass to the model at a time.\n\nblock_size = 8\nprint(f\"For context(or input) {train_data[:block_size]}, next word to be predicted(or target) is {train_data[block_size]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:16:33.189260Z","iopub.execute_input":"2025-10-26T19:16:33.189909Z","iopub.status.idle":"2025-10-26T19:16:33.196609Z","shell.execute_reply.started":"2025-10-26T19:16:33.189883Z","shell.execute_reply":"2025-10-26T19:16:33.195552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### This solves the time dimension of the problem. Next, our job is to solve for batches to process parallely.","metadata":{}},{"cell_type":"code","source":"# Basically takes any 4 points from the entire training data\ntorch.randint(len(data)-block_size, (batch_size,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:28:41.314072Z","iopub.execute_input":"2025-10-26T19:28:41.314400Z","iopub.status.idle":"2025-10-26T19:28:41.322119Z","shell.execute_reply.started":"2025-10-26T19:28:41.314378Z","shell.execute_reply":"2025-10-26T19:28:41.321070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(1224)\n\nbatch_size = 4   # of sentence blocks to be processed\nblock_size = 8   # of tokens in each sentence block\n\n# We will now create a tensor of size (batch_size x block_size) for both x and y\ndef get_batch(data):\n    idx = torch.randint(len(data)-block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in idx])      # stack() is faster when we already have a pytorch tensor\n    target_list = [data[i+1:i+block_size+1] for i in idx]           # Another way of stacking the batches\n    y = torch.stack(target_list)\n    return x, y\n\ntrain_xb, train_yb = get_batch(train_data)\nprint(\"Input sequence:\\n\",train_xb,\"\\nNext target tokens:\\n\", train_yb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:50:12.806974Z","iopub.execute_input":"2025-10-26T19:50:12.807307Z","iopub.status.idle":"2025-10-26T19:50:12.817965Z","shell.execute_reply.started":"2025-10-26T19:50:12.807283Z","shell.execute_reply":"2025-10-26T19:50:12.816798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### To be read as: \nFor n elements of any input sequence in the batch, the output should be the nth element of the target sequence generated.","metadata":{}},{"cell_type":"code","source":"for b in range(batch_size):         # Iterating through batches\n    for t in range(block_size):     # Iterating through each sequence\n        context = train_xb[b, :t+1]      # First t elements of each batch\n        target = train_yb[b, t]          # (t)th element of the same batch from the target tensor\n        print(context, \" --> \", target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T20:00:07.813779Z","iopub.execute_input":"2025-10-26T20:00:07.814894Z","iopub.status.idle":"2025-10-26T20:00:07.829782Z","shell.execute_reply.started":"2025-10-26T20:00:07.814855Z","shell.execute_reply":"2025-10-26T20:00:07.828737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We will first train the simplest neural network which uses the last token to predict the next one by using the probability of the combination of words occurring given the first word occurs(basic conditional prob.) and is called transitional probability.\n* *P(cat | the) = count(the, cat) / count(the)*","metadata":{"execution":{"iopub.status.busy":"2025-10-26T20:05:30.470403Z","iopub.execute_input":"2025-10-26T20:05:30.471367Z","iopub.status.idle":"2025-10-26T20:05:30.486155Z","shell.execute_reply.started":"2025-10-26T20:05:30.471319Z","shell.execute_reply":"2025-10-26T20:05:30.484591Z"}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BiGramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):          # Creating a lookup table of probabilities of each token to come after the current one\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)           # Embedding layer creates the table of size nxn if there are n tokens in the data\n\n    def forward(self, idx, targets=None):\n        '''\n        This function fetches the pobabilities of the current batch of words from the table,\n        takes the one with highest probability and flattens it to calculate CE loss.\n        Flattening makes CE loss calculation faster since individual errors are not being calculated.\n        '''\n        \n        logits = self.token_embedding_table(idx)        # Fetching the probabilities of next words for each token in the current batch\n        \n        if targets is not None:\n            B, T, C = logits.shape\n            logits_flat = logits.view(B*T, C)           # Flattening the probabilities of next words accross batches and timesteps\n            targets_flat = targets.view(B*T)            # Flattening the probability of the target\n            loss = F.cross_entropy(logits, targets)     # Use of functional module here\n        else:\n            loss = None\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:49:42.351049Z","iopub.execute_input":"2025-10-26T21:49:42.356388Z","iopub.status.idle":"2025-10-26T21:49:42.378273Z","shell.execute_reply.started":"2025-10-26T21:49:42.356304Z","shell.execute_reply":"2025-10-26T21:49:42.376618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}