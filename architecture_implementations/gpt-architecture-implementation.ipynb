{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef60427",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:51.844702Z",
     "iopub.status.busy": "2025-10-30T22:29:51.844379Z",
     "iopub.status.idle": "2025-10-30T22:29:53.717709Z",
     "shell.execute_reply": "2025-10-30T22:29:53.716664Z"
    },
    "papermill": {
     "duration": 1.880548,
     "end_time": "2025-10-30T22:29:53.719154",
     "exception": false,
     "start_time": "2025-10-30T22:29:51.838606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gpt-training-data/input.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885a7d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.727428Z",
     "iopub.status.busy": "2025-10-30T22:29:53.727043Z",
     "iopub.status.idle": "2025-10-30T22:29:53.748106Z",
     "shell.execute_reply": "2025-10-30T22:29:53.747267Z"
    },
    "papermill": {
     "duration": 0.026843,
     "end_time": "2025-10-30T22:29:53.749797",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.722954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/gpt-training-data/input.txt', 'r', encoding='utf-8') as f:   #utf-8 enables reading of special characters that are skipped otherwise\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bfbf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.758427Z",
     "iopub.status.busy": "2025-10-30T22:29:53.757702Z",
     "iopub.status.idle": "2025-10-30T22:29:53.763894Z",
     "shell.execute_reply": "2025-10-30T22:29:53.763128Z"
    },
    "papermill": {
     "duration": 0.01163,
     "end_time": "2025-10-30T22:29:53.765125",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.753495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)    #no. of characters we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecfe9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.773661Z",
     "iopub.status.busy": "2025-10-30T22:29:53.773342Z",
     "iopub.status.idle": "2025-10-30T22:29:53.778317Z",
     "shell.execute_reply": "2025-10-30T22:29:53.777346Z"
    },
    "papermill": {
     "duration": 0.010852,
     "end_time": "2025-10-30T22:29:53.779635",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.768783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a945b431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.787855Z",
     "iopub.status.busy": "2025-10-30T22:29:53.787594Z",
     "iopub.status.idle": "2025-10-30T22:29:53.804855Z",
     "shell.execute_reply": "2025-10-30T22:29:53.804046Z"
    },
    "papermill": {
     "duration": 0.022728,
     "end_time": "2025-10-30T22:29:53.806085",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.783357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))   #to see all the characters the model is going to encounter\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b661816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.814561Z",
     "iopub.status.busy": "2025-10-30T22:29:53.813839Z",
     "iopub.status.idle": "2025-10-30T22:29:53.821094Z",
     "shell.execute_reply": "2025-10-30T22:29:53.820370Z"
    },
    "papermill": {
     "duration": 0.012878,
     "end_time": "2025-10-30T22:29:53.822513",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.809635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 39, 39, 39, 40, 40, 40, 40]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Next step is to tokenize the text.\n",
    "Simple baseline tokenization would be taking the ascii value of each character\n",
    "and keeping all the values together in a list to create a vector.\n",
    "Then use libraries like tiktoken and more complex tokenizers.\n",
    "'''\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]    #Lambda function just applies the encoding to all the strings\n",
    "decode = lambda l: ''.join([itos[i] for i in l])    #This lambda function does the reverse.\n",
    "\n",
    "encode('aaaabbbb')\n",
    "# decode(encode('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db21f419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:53.831478Z",
     "iopub.status.busy": "2025-10-30T22:29:53.830677Z",
     "iopub.status.idle": "2025-10-30T22:29:58.675612Z",
     "shell.execute_reply": "2025-10-30T22:29:58.674509Z"
    },
    "papermill": {
     "duration": 4.850849,
     "end_time": "2025-10-30T22:29:58.677238",
     "exception": false,
     "start_time": "2025-10-30T22:29:53.826389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)   #To simply create a pytorch tensor for entering into model training.\n",
    "data.shape\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82399c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.686586Z",
     "iopub.status.busy": "2025-10-30T22:29:58.686107Z",
     "iopub.status.idle": "2025-10-30T22:29:58.706142Z",
     "shell.execute_reply": "2025-10-30T22:29:58.705441Z"
    },
    "papermill": {
     "duration": 0.026411,
     "end_time": "2025-10-30T22:29:58.707692",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.681281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We'll now split into training and validation data for the model\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4f9ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.716621Z",
     "iopub.status.busy": "2025-10-30T22:29:58.716256Z",
     "iopub.status.idle": "2025-10-30T22:29:58.742955Z",
     "shell.execute_reply": "2025-10-30T22:29:58.741735Z"
    },
    "papermill": {
     "duration": 0.032998,
     "end_time": "2025-10-30T22:29:58.744626",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.711628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For context(or input) tensor([18, 47, 56, 57, 58,  1, 15, 47]), next word to be predicted(or target) is 58\n"
     ]
    }
   ],
   "source": [
    "#We'll define the block size of characters that we pass to the model at a time.\n",
    "\n",
    "block_size = 8\n",
    "print(f\"For context(or input) {train_data[:block_size]}, next word to be predicted(or target) is {train_data[block_size]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf7f656",
   "metadata": {
    "papermill": {
     "duration": 0.003574,
     "end_time": "2025-10-30T22:29:58.752270",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.748696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This solves the time dimension of the problem. Next, our job is to solve for batches to process parallely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49911bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.761623Z",
     "iopub.status.busy": "2025-10-30T22:29:58.760859Z",
     "iopub.status.idle": "2025-10-30T22:29:58.799963Z",
     "shell.execute_reply": "2025-10-30T22:29:58.798813Z"
    },
    "papermill": {
     "duration": 0.045574,
     "end_time": "2025-10-30T22:29:58.801655",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.756081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      " tensor([[27, 24, 13, 26, 33, 31, 10,  0],\n",
      "        [53, 51, 43,  6,  1, 39, 52, 42],\n",
      "        [43,  1, 40, 59, 58,  0, 24, 53],\n",
      "        [63, 53, 59,  1, 51, 53, 56, 43]]) \n",
      "Next target tokens:\n",
      " tensor([[24, 13, 26, 33, 31, 10,  0, 26],\n",
      "        [51, 43,  6,  1, 39, 52, 42,  6],\n",
      "        [ 1, 40, 59, 58,  0, 24, 53, 53],\n",
      "        [53, 59,  1, 51, 53, 56, 43,  1]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1224)\n",
    "\n",
    "batch_size = 4   # of sentence blocks to be processed\n",
    "block_size = 8   # of tokens in each sentence block\n",
    "\n",
    "# We will now create a tensor of size (batch_size x block_size) for both x and y\n",
    "def get_batch(data):\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])      # stack() is faster when we already have a pytorch tensor\n",
    "    target_list = [data[i+1:i+block_size+1] for i in idx]           # Another way of stacking the batches\n",
    "    y = torch.stack(target_list)\n",
    "    return x, y\n",
    "\n",
    "train_xb, train_yb = get_batch(train_data)\n",
    "print(\"Input sequence:\\n\",train_xb,\"\\nNext target tokens:\\n\", train_yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a803c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.811318Z",
     "iopub.status.busy": "2025-10-30T22:29:58.810626Z",
     "iopub.status.idle": "2025-10-30T22:29:58.817164Z",
     "shell.execute_reply": "2025-10-30T22:29:58.816440Z"
    },
    "papermill": {
     "duration": 0.012966,
     "end_time": "2025-10-30T22:29:58.818626",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.805660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([589516, 373786, 401102, 717225])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basically takes any 4 points from the entire training data\n",
    "torch.randint(len(data)-block_size, (batch_size,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0250526",
   "metadata": {
    "papermill": {
     "duration": 0.00389,
     "end_time": "2025-10-30T22:29:58.826642",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.822752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### To be read as: \n",
    "For n elements of any input sequence in the batch, the output should be the nth element of the target sequence generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978687c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.835861Z",
     "iopub.status.busy": "2025-10-30T22:29:58.835534Z",
     "iopub.status.idle": "2025-10-30T22:29:58.849234Z",
     "shell.execute_reply": "2025-10-30T22:29:58.848018Z"
    },
    "papermill": {
     "duration": 0.020239,
     "end_time": "2025-10-30T22:29:58.850879",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.830640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27])  -->  tensor(24)\n",
      "tensor([27, 24])  -->  tensor(13)\n",
      "tensor([27, 24, 13])  -->  tensor(26)\n",
      "tensor([27, 24, 13, 26])  -->  tensor(33)\n",
      "tensor([27, 24, 13, 26, 33])  -->  tensor(31)\n",
      "tensor([27, 24, 13, 26, 33, 31])  -->  tensor(10)\n",
      "tensor([27, 24, 13, 26, 33, 31, 10])  -->  tensor(0)\n",
      "tensor([27, 24, 13, 26, 33, 31, 10,  0])  -->  tensor(26)\n",
      "tensor([53])  -->  tensor(51)\n",
      "tensor([53, 51])  -->  tensor(43)\n",
      "tensor([53, 51, 43])  -->  tensor(6)\n",
      "tensor([53, 51, 43,  6])  -->  tensor(1)\n",
      "tensor([53, 51, 43,  6,  1])  -->  tensor(39)\n",
      "tensor([53, 51, 43,  6,  1, 39])  -->  tensor(52)\n",
      "tensor([53, 51, 43,  6,  1, 39, 52])  -->  tensor(42)\n",
      "tensor([53, 51, 43,  6,  1, 39, 52, 42])  -->  tensor(6)\n",
      "tensor([43])  -->  tensor(1)\n",
      "tensor([43,  1])  -->  tensor(40)\n",
      "tensor([43,  1, 40])  -->  tensor(59)\n",
      "tensor([43,  1, 40, 59])  -->  tensor(58)\n",
      "tensor([43,  1, 40, 59, 58])  -->  tensor(0)\n",
      "tensor([43,  1, 40, 59, 58,  0])  -->  tensor(24)\n",
      "tensor([43,  1, 40, 59, 58,  0, 24])  -->  tensor(53)\n",
      "tensor([43,  1, 40, 59, 58,  0, 24, 53])  -->  tensor(53)\n",
      "tensor([63])  -->  tensor(53)\n",
      "tensor([63, 53])  -->  tensor(59)\n",
      "tensor([63, 53, 59])  -->  tensor(1)\n",
      "tensor([63, 53, 59,  1])  -->  tensor(51)\n",
      "tensor([63, 53, 59,  1, 51])  -->  tensor(53)\n",
      "tensor([63, 53, 59,  1, 51, 53])  -->  tensor(56)\n",
      "tensor([63, 53, 59,  1, 51, 53, 56])  -->  tensor(43)\n",
      "tensor([63, 53, 59,  1, 51, 53, 56, 43])  -->  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):         # Iterating through batches\n",
    "    for t in range(block_size):     # Iterating through each sequence\n",
    "        context = train_xb[b, :t+1]      # First t elements of each batch\n",
    "        target = train_yb[b, t]          # (t)th element of the same batch from the target tensor\n",
    "        print(context, \" --> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc30645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T20:05:30.471367Z",
     "iopub.status.busy": "2025-10-26T20:05:30.470403Z",
     "iopub.status.idle": "2025-10-26T20:05:30.486155Z",
     "shell.execute_reply": "2025-10-26T20:05:30.484591Z",
     "shell.execute_reply.started": "2025-10-26T20:05:30.471319Z"
    },
    "papermill": {
     "duration": 0.003705,
     "end_time": "2025-10-30T22:29:58.858830",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.855125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### We will first train the simplest neural network which uses the last token to predict the next one by using the probability of the combination of words occurring given the first word occurs(basic conditional prob.) and is called transitional probability.\n",
    "* *P(cat | the) = count(the, cat) / count(the)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6ad3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.868471Z",
     "iopub.status.busy": "2025-10-30T22:29:58.868019Z",
     "iopub.status.idle": "2025-10-30T22:29:58.978851Z",
     "shell.execute_reply": "2025-10-30T22:29:58.977656Z"
    },
    "papermill": {
     "duration": 0.117373,
     "end_time": "2025-10-30T22:29:58.980352",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.862979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor(4.5888, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Wp;aW:gCxuydpHjFvNsLQJODktnALvnIDlfYUwR':ABBvR'RUj\n",
      " aisIiQKliM,;sK!VtwP'oyVVkXpWS3X,NHYsbp?Tp3DMXI,3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):          # Creating a lookup table of probabilities of each token to come after the current one\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)           # Embedding layer creates the table of size nxn if there are n tokens in the data\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        '''\n",
    "        This function fetches the pobabilities of the current batch of words from the table,\n",
    "        takes the one with highest probability and flattens it to calculate CE loss.\n",
    "        Flattening makes CE loss calculation faster since individual errors are not being calculated.\n",
    "        '''\n",
    "        \n",
    "        logits = self.token_embedding_table(idx)        # Fetching the probabilities of next words for each token in the current batch\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B*T, C)           # Flattening the probabilities of next words accross batches and timesteps\n",
    "            targets_flat = targets.view(B*T)            # Flattening the probability of the target\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)     # Use of functional module here\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for i in range(max_tokens):\n",
    "            logits, loss = self(idx) # Or self.forward(idx) - Calling an instance of a class activates the __call__ method and in the nn.Module class, this call further calls the forward function.\n",
    "            logits = logits[:, -1, :]                       # Taking only the last time step as it is a bi-gram model\n",
    "            prob = F.softmax(logits, dim = -1)              # We create probability from likelihood values in the embedding table in order to normalize it and get actual chances.\n",
    "            next_word = torch.multinomial(prob, num_samples = 1)    # Sampling ensures that we don't generate the same answer for a word always. Argmax on the other hand takes the token with max. prob. which removes variability\n",
    "            idx = torch.cat((idx, next_word), dim = 1)      # Appending the newly generated word to the current sequence for further prediction if reqd.\n",
    "        \n",
    "            # This loop repeats this generation till the token limit is reached and returns the final sequence\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BiGramLanguageModel(vocab_size)\n",
    "logits, loss = model(train_xb, train_yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80695a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T22:29:58.989646Z",
     "iopub.status.busy": "2025-10-30T22:29:58.989362Z",
     "iopub.status.idle": "2025-10-30T22:29:58.993207Z",
     "shell.execute_reply": "2025-10-30T22:29:58.992425Z"
    },
    "papermill": {
     "duration": 0.010203,
     "end_time": "2025-10-30T22:29:58.994632",
     "exception": false,
     "start_time": "2025-10-30T22:29:58.984429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the rest of the model using optimizers etc."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8572763,
     "sourceId": 13502012,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.208366,
   "end_time": "2025-10-30T22:30:01.453381",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-30T22:29:47.245015",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
